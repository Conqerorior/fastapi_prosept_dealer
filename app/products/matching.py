# -*- coding: utf-8 -*-
"""просепт_хакатон (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HQC1SQGhc0aI_Gg3tSyrlmOPsSjTGnc5
"""


def matching(lst_dict_pr,
             lst_dict_dr):  # lst_dict_pr список словарей карточек производителя
    # lst_dict_dr список словарей карточек диллеров
    import re

    import nltk
    import numpy as np
    import pandas as pd
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    from scipy.spatial.distance import cdist
    from sklearn.feature_extraction.text import TfidfVectorizer

    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('wordnet')

    data_mdp = pd.DataFrame(lst_dict_dr)
    data_mp = pd.DataFrame(lst_dict_pr)

    data_mp.dropna(subset=['name'],
                   inplace=True)  # избавляемся от пропусков в столбце name
    data_mp.reset_index(drop=True, inplace=True)

    data_mp_name = data_mp[['id', 'name']]

    def separation(
            s):  # функция для удаления знаков препинания, а также для расклеевания склеенных слов,
        # то есть разделения пробелами латиницы от кириллицы,
        # чисел от букв и слов в верхнем регистре от слов в нижнем регистре в склеенных словах.
        s = re.sub(r'[^\w\s]', ' ', s)
        lat = re.findall(r"[a-zA-Z]+", s)
        num = re.findall(r"\d+", s)
        up = re.findall(r"[А-Я]+", s)
        for i in lat:
            s = s.replace(i, ' ' + i + ' ')
        for i in num:
            s = s.replace(i, ' ' + i + ' ')
        for i in up:
            if len(i) > 1:
                s = s.replace(i, ' ' + i + ' ')
        s = ' '.join([i.lower() for i in s.split()])
        return s

    stopword_en = stopwords.words('english')
    stopword_ru = stopwords.words('russian')

    porter = PorterStemmer()
    lemmatizer = WordNetLemmatizer()

    def tokenize(df,
                 name):  # функция для подготовки к векторизации: разделение склеенных слов,
        # токенизация, лемматизация, стем, избавление от стоп-слов,
        # приведение к нижнему регистру
        tokenized = []
        for q in df[name]:
            q = separation(q)
            q = word_tokenize(q)
            q = [lemmatizer.lemmatize(i) for i in q if
                 (i not in stopword_en) and
                 (i not in stopword_ru)]
            q = [porter.stem(i) for i in q]
            tokenized.append(q)
        name_tok = []
        for i in tokenized:
            name_tok.append(' '.join(i))
        df[name + '_tok'] = name_tok
        return df

    tokenize(data_mp_name,
             'name')  # функция tokenize для данных от производителя

    data_mp_id = data_mp_name.loc[:, ['id', 'name_tok']]
    data_mp_name.drop(['name', 'id'], axis=1, inplace=True)

    tfIdfVectorizer = TfidfVectorizer(use_idf=True)
    tfIdf = tfIdfVectorizer.fit_transform(
        data_mp_name['name_tok'])  # векторизация данных производителя

    tokenize(data_mdp, 'product_name')  # функция tokenize для данных от дилеров

    tfidf_test = tfIdfVectorizer.transform(
        data_mdp['product_name_tok'])  # векторизация данных от дилеров

    res = cdist(tfidf_test.toarray(), tfIdf.toarray(),
                metric='euclidean')  # рассчёт расстояний
    res = pd.DataFrame(res)
    res_5 = pd.DataFrame([np.argsort(res.iloc[i, :])[:5] for i in range(
        res.shape[0])])  # 5 ближайших соседей (по индексам)
    df_res_5 = pd.DataFrame(
        [[data_mp_id.loc[i, 'id'] for i in res_5.loc[j, :]] for j in
         range(res_5.shape[0])], \
        columns=['1', '2', '3', '4',
                 '5'])  # 5 ближайших соседей (по id карточек производителя)

    res_5 = df_res_5.to_dict('records')

    return res_5
